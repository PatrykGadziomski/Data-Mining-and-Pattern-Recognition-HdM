{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Versuch Fahrzeugdaten\n",
    "\n",
    "* Autor: Prof. Dr. Johannes Maucher\n",
    "\n",
    "## Abgabe:\n",
    "\n",
    "- **Abzugeben ist das Jupyter Notebook mit dem verlangten Implementierungen und den entsprechenden Ausgaben.**\n",
    "- **Das Notebook ist als .ipynb und als .html abzugeben.**\n",
    "- **Klausurelevante Fragen sind Dokument \"Fragenkatalog Datamining\" zu finden.**\n",
    "- Antworten auf Fragen im Notebook, Diskussionen und Beschreibung der Ergebnisse sind optional (aber empfohlen) und werden nicht bewertet.\n",
    "\n",
    "* [Übersicht Data Mining Praktikum](https://maucher.pages.mi.hdm-stuttgart.de/ai/page/dm/)\n",
    "\n",
    "\n",
    "# Einführung\n",
    "\n",
    "## Lernziele:\n",
    "\n",
    "In diesem Versuch sollen Kenntnisse in folgenden Themen vermittelt werden:\n",
    "\n",
    "* Datenimport und Datenexport von und zu \n",
    "    * Pandas Dataframes\n",
    "    * PostgreSQL Datenbanken\n",
    "* Explorative Datenanalysen (EDA)\n",
    "* Datenvisualisierung mit Matplotlib und plotly\n",
    "* Überwachtes Lernen eines Klassifikationsmodells\n",
    "* Überwachtes Lernen eines Regressionsmodells\n",
    "* Evaluation von Klassifikationsmodellen\n",
    "* Evaluation von Regressionsmodellen\n",
    "* Kreuzvalidierung\n",
    "* Hyperparameteroptimierung\n",
    "\n",
    "## Vorbereitung\n",
    "\n",
    "### Datenbankzugriff\n",
    "\n",
    "1. Installieren Sie PostgreSQL. Mit PostgreSQL sollte auch pgAdmin installiert werden. PgAdmin ist eine open-source Software für die Entwicklung und die Administration von PostgreSQL Datenbanken.\n",
    "2. Legen Sie über pgAdmin eine Datenbank für das Datamining-Praktikum an. In diese Datenbank werden alle in diesem Versuch relevanten Tabellen geschrieben.\n",
    "3. Für den Datenbankzugriff aus Python heraus wird in diesem Versuch [SQLAlchemy](http://docs.sqlalchemy.org/en/latest/intro.html) eingesetzt. Machen Sie sich mit den Basics von SQLAlchemy vertraut, z.B. mithilfe von [https://maucher.pages.mi.hdm-stuttgart.de/python4datascience/07DataBaseSQL.html#using-sqlalchemy-and-pandas](https://maucher.pages.mi.hdm-stuttgart.de/python4datascience/07DataBaseSQL.html#using-sqlalchemy-and-pandas), Abschnitt *Using SQLAlchemy and Pandas*.\n",
    "\n",
    "### Pandas Dataframe\n",
    "\n",
    "Machen Sie sich mit den Grundlagen von Pandas vertraut.\n",
    "\n",
    "\n",
    "### Machine Learning\n",
    "\n",
    "Machen Sie sich mit Entscheidungsbäumen, Random Forest, Single Layer Perzeptron und Multi Layer Perzeptron vertraut. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Durchführung\n",
    "\n",
    "## Einlesen der Daten aus .csv und Ablage in PostgreSQL\n",
    "In diesem ersten Teil des Versuchs sollen die relevanten Daten aus dem .csv-File eingelesen und in einer PostgreSQL-Tabelle abgelegt werden. Das benötigte File `Fahrzeuginformationen.csv` liegt im aktuellen Verzeichnis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Laden Sie die .csv-Datei in einen Pandas Dataframe. \n",
    "\n",
    "2. Zeigen Sie für den angelegten Dataframe \n",
    "    * die ersten 10 Zeilen\n",
    "    * die Größe (Anzahl Zeilen und Anzahl Spalten)\n",
    "    * die Anzahl der NaNs pro Spalte\n",
    "    an. \n",
    "3. Zeigen Sie mit der Pandas-Dataframe Methode `info()`, den Datentyp aller Spalten an. Der Typ der Spalte `CO2-Emissionen` ist tatsächlich kein numerischer Typ. Finden Sie heraus warum das so ist. Beheben Sie den *Fehler* und sorgen Sie dafür, dass auch diese Spalte einen numerischen Typ hat.\n",
    "\n",
    "4. Schreiben Sie den im vorigen Schritt angepassten Dataframe mit der Pandas Methode `to_sql()` in eine Datenbanktabelle mit dem Namen `vehicledata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'..\\Experiment 1\\Fahrzeuginformationen.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rows:', len(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Columns', len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['CO2-Emissionen'].astype(float)\n",
    "# The values in column 'CO2-Emission' are identified as object values because some of the values contains commata ','\n",
    "# Solution: CONvert the value to floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_co2_emission = df['CO2-Emissionen']\n",
    "df_co2_emission = [i.replace(',', '.') for i in df_co2_emission]\n",
    "df['CO2-Emission_NEW'] = df_co2_emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CO2-Emission_NEW'] = df['CO2-Emission_NEW'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -y psycopg2\n",
    "#!conda install -y -c anaconda sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, inspect, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # connection\n",
    "# conn_str = 'postgresql://admin:admin1234@localhost:5432/postgres'\n",
    "# engine = create_engine(conn_str).connect()\n",
    "# print(engine)\n",
    "\n",
    "# # # check if a table exists\n",
    "# # inspec = inspect(engine)\n",
    "# # print(inspec.has_table(\"vehicledata\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_sql('vehicleData.sql', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_query = '''\n",
    "# COPY vehicleData(...)\n",
    "# FROM \"\"\n",
    "# DELIMETER \",\"\n",
    "# CSV HEADER;\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplarische Datenbankabfragen\n",
    "\n",
    "1. Verwenden Sie Pandas Dataframe Methode `read_sql_query()` um 3 für Sie interessante Datenbankabfragen zu implementieren. Die Resultate der Abfragen werden in einen Pandas Dataframe geschrieben. Zeigen Sie diese an. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f85ce87e-60b5-47a0-9c0e-dec064834222",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Exploration\n",
    "1. Zeigen Sie für alle Spalten die Anzahl der unterschiedlichen Werte in dieser Spalte an.\n",
    "2. Benutzen Sie die Pandas Dataframe Methode `describe()` um sämtliche deskriptiven Statistiken anzuzeigen.\n",
    "3. Legen Sie eine Liste `numeric_features` an, welche nur die Spaltennamen der numerischen Spalten enthält.\n",
    "4. Schreiben Sie die Namen aller nicht-numerischen Spalten in eine Liste `categoric_features`.\n",
    "5. Visualisieren Sie für die Spalten `HST_Benennung`, `Neupreis Brutto`, `CO2-Emissionen` und `Produktgruppe` die Verteilung der Werte in einem Barplot bzw. Histogramm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df:\n",
    "    print(df[i].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_numeric_dtype\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [i for i in df.columns.values if is_numeric_dtype(df[i]) == True]\n",
    "numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric_features = [i for i in df.columns.values if is_numeric_dtype(df[i]) == False]\n",
    "categoric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisieren Sie für die Spalten `HST_Benennung`, `Neupreis Brutto`, `CO2-Emissionen` und `Produktgruppe` die Verteilung der Werte in einem Barplot bzw. Histogramm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_for_plot = df[['HST Benennung', 'Neupreis Brutto', 'CO2-Emission_NEW', 'Produktgruppe']]\n",
    "df_for_plot1 = df[['HST Benennung']].value_counts()\n",
    "df_for_plot2 = df[['Neupreis Brutto']].value_counts()\n",
    "df_for_plot3 = df[['CO2-Emission_NEW']].value_counts()\n",
    "df_for_plot4 = df[['Produktgruppe']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "df_for_plot1.plot(ax=axes[0], kind='bar', title='HST Benennung / BAR', figsize=(15, 10))\n",
    "df_for_plot1.plot(ax=axes[1], kind='hist', title='HST Benennung / HIST', figsize=(15, 10))\n",
    "\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig2, axes = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "# df_for_plot2.plot(ax=axes[0], kind='bar', title='Neupreis Brutto / BAR', figsize=(15, 10))\n",
    "# df_for_plot2.plot(ax=axes[1], kind='hist', title='Neupreis Brutto / HIST', figsize=(15, 10))\n",
    "\n",
    "# fig2.set_figheight(5)\n",
    "# fig2.set_figwidth(15)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, axes = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "df_for_plot3.plot(ax=axes[0], kind='bar', title='CO2 Emission / BAR', figsize=(15, 10))\n",
    "df_for_plot3.plot(ax=axes[1], kind='hist', title='CO2 Emission / HIST', figsize=(15, 10))\n",
    "\n",
    "fig3.set_figheight(5)\n",
    "fig3.set_figwidth(15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4, axes = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "df_for_plot4.plot(ax=axes[0], kind='bar', title='Produktgruppe / BAR', figsize=(15, 10))\n",
    "df_for_plot4.plot(ax=axes[1], kind='hist', title='Produktgruppe / HIST', figsize=(15, 10))\n",
    "\n",
    "fig4.set_figheight(5)\n",
    "fig4.set_figwidth(15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning 1: Produktgruppenbestimmung\n",
    "\n",
    "In diesem Abschnitt soll ein Klassifikator trainiert werden, welcher anhand von Eingabemerkmalen, wie *Breite*, *Höhe*, *Gewicht* usw. das zugehörige Fahrzeugsegment (`Produktgruppe`) vorhersagt.\n",
    "\n",
    "In diesem Teilversuch sollen als Eingabemerkmale die zuvor in `numeric_features` definierten Spalten und die nicht-numerischen Spalten `Antrieb`, `Kraftstoffart`, `KSTA Motor` verwendet werden. Die Zielvariable (Ausgabe) stellt die Spalte `Produktgruppe` dar.\n",
    "\n",
    "\n",
    "### Produktgrunppenspezifische Visualisierung\n",
    "\n",
    "1. Plotten Sie für die drei oben angegebenen nicht-numerischen Merkmale jeweils eine Produktgruppen-spezifische Häufigkeitsverteilung in der unten dargestellten Form. \n",
    "\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/antrieb_produktgruppe.png\" style=\"width:500px\" align=\"center\">\n",
    "\n",
    "2. Plotten Sie für alle numerischen Merkmale jeweils einen Produktgruppen-spezifischen Boxplot in der unten dargestellten Form. \n",
    "\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/neupreis_produktgruppe.png\" style=\"width:500px\" align=\"center\">\n",
    "\n",
    "3. Erzeugen Sie mit [plotly.express scatter()](https://plotly.com/python/line-and-scatter/) einen 2-dimensionalen Plot, in dem alle Fahrzeuge wie folgt dargestellt werden (pro Fahrzeug ein Marker):\n",
    "- x-Achse: `Länge`\n",
    "- y-Achse: `Höhe`\n",
    "- Farbe des Markers: `Produktgruppe`\n",
    "- Größe des Markers: `Leergewicht`\n",
    "- Bei *Mouse-Over* soll für den jeweiligen Marker der entsprechende Wert von `Neupreis Brutto` und `HST-HT Benennung` angezeigt werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "antrieb = df.groupby(['Antrieb', 'Produktgruppe']).size().reset_index(name='count')\n",
    "antrieb['Antrieb'] = antrieb['Antrieb'].map(lambda x: x.strip())\n",
    "\n",
    "antrieb_A = antrieb[antrieb['Antrieb'] == 'A']\n",
    "antrieb_FA = antrieb[antrieb['Antrieb'] == 'FA']\n",
    "antrieb_HA = antrieb[antrieb['Antrieb'] == 'HA']\n",
    "\n",
    "# antrieb_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.barh(antrieb_A['Produktgruppe'], antrieb_A['count'], color='r')\n",
    "plt.barh(antrieb_HA['Produktgruppe'], antrieb_HA['count'], color='y')\n",
    "plt.barh(antrieb_FA['Produktgruppe'], antrieb_FA['count'], color='b')\n",
    "\n",
    "plt.legend(['A', 'FA', 'HA'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "63e5b2ec-17c0-4494-b5ab-a8a7a203d881",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data Encoding\n",
    "\n",
    "1. Categoriale Merkmale ohne Ordnungsrelation (=nominale Merkmale) müssen One-Hot-Encodiert werden. Führen Sie für die drei categorialen Merkmale ein One-Hot-Encoding mit dem [scikit-learn LabelBinarizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html) durch.\n",
    "2. Fügen Sie die one-hot-encodierten Spalten mit den numerischen Spalten zusammen. Weisen Sie die entsprechende Eingabedatenmatrix einem 2-dimensionalen numpy-array `X` zu. \n",
    "3. Führen Sie auf die Zielvariable `Produktgruppe` ein Label-Encoding mit [scikit-learn LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder) aus. Weisen Sie diese Daten dem 1-dimensionalen numpy-array `y` zu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1535bdf1-f3a9-4a57-88d2-756af7b52be8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Generate Training- and Testpartition\n",
    "Benutzen Sie die [scikit-learn Methode train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) um `X` und `y` in einer Trainings- und Testpartition aufzuteilen. 30% der Daten soll für das Testen, 70% für das Training benutzt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b8a8c080-f89e-4b8d-b765-4d6a8ba1c863",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Decision Tree Training, Test and Evaluation\n",
    "1. Trainieren Sie einen [Entscheidungsbaum](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) mit den Trainingsdaten.\n",
    "2. Wenden Sie den gelernten Entscheidungsbaum auf die Testdaten an.\n",
    "3. Evaluieren Sie die Qualität des Entscheidungsbaumes indem Sie \n",
    "     - einen [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) erzeugen. \n",
    "     - die [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) plotten.\n",
    "4. Interpretieren Sie das Ergebnis.\n",
    "5. Führen Sie eine [10-fache Kreuzvalidierung](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) des Entscheidungsbaumes mit den Daten `X` und `y` aus. Interpretieren Sie das Ergebnis.\n",
    "6. Bestimmen Sie die *Wichtigkeit* der Eingabemerkmale für die Klassifikationsaufgabe, indem Sie auf den in 1.) gelernten DecisionTree das Attribut `feature_importance_` abfragen. Stellen Sie die Werte in einem Barplot dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "af044466-8c3e-4681-b5e2-c248cae1321d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Random Forest Training, Test and Evaluation\n",
    "Wiederholen Sie die Teilaufgaben 1. bis 5. des Entscheidungsbaums für einen [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). Vergelichen Sie die Performance der beiden Verfahren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning 2: Schätzung der CO2-Emission\n",
    "In diesem Teilversuch soll aus den Eingabemerkmalen \n",
    "\n",
    "`\"CCM\",\"HST PS\", \"Anzahl der Türen\", \"Leergewicht\", \"Zuladung\", \"Länge\", \"Breite\", \"Höhe\"`\n",
    "\n",
    "die Zielvariable \n",
    "\n",
    "`CO2-Emissionen`\n",
    "\n",
    "geschätzt werden. Hierzu soll ein möglichst gutes Regressionsmodell trainiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visuelle Korrelationsanalyse\n",
    "1. Stellen Sie für jedes der 8 Eingabemerkmale die Korrelation mit der Zielvariablen visuell in einem Scatterplot dar, in dem das jeweilige Eingabemerkmal auf der x-Achse und die Zielvariable auf der y-Achse aufgetragen wird.\n",
    "2. Diskutieren Sie die Korrelationen. Welche Merkmale korrelieren am stärksten mit der Zielvariable? Erscheint Ihnen das plausibel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Encoding\n",
    "1. Weisen Sie die Matrix der Eingabedaten dem 2-dimensionalen Array `X` und die Zielvariable dem 1-dimensionalen Array `y` zu.\n",
    "2. Führen Sie auf `X` und `y` eine Partitionierung in Trainings- und Testdaten durch, wieder im Verhältnis 70/30.\n",
    "3. Skalieren Sie die Eingabevariablen und die Zielvariable mit dem [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html). Die Skalierung muss sowohl auf Trainings- als auch auf Testdaten ausgeführt werden. Warum darf die Skalierung erst nach dem Split in die beiden Partitionen ausgeführt werden? Worauf ist zu achten? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Test und Evaluation verschiedener Regressionsmodelle\n",
    "\n",
    "Führen Sie die folgenden Teilaufgaben sowohl für ein [Single Layer Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) als auch für ein [Multi Layer Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) mit 20 Neuronen in der Hidden-Schicht durch. Vergleichen Sie am Ende die Performance der beiden Verfahren.\n",
    "1. Trainieren Sie den Algorithmus mit den Trainingsdaten.\n",
    "2. Wenden Sie das gelernte Modell auf die Testdaten an.\n",
    "3. Evaluieren Sie die Qualität der Modelle, indem Sie auf die vorhergesagten Ausgaben und die wahren Ausgaben die unten gegebene Funktion aufrufen.\n",
    "4. Beschreiben Sie kurz die in der Funktion verwendeten Metriken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determineRegressionMetrics(y_test,y_pred,title=\"\"):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mad = mean_absolute_error(y_test, y_pred)\n",
    "    rmsle=np.sqrt(mean_squared_error(np.log(y_test+1),np.log(y_pred+1)))# +1 for avoiding log(0) \n",
    "    r2=r2_score(y_test, y_pred)\n",
    "    med=median_absolute_error(y_test, y_pred)\n",
    "    print(title)\n",
    "    print(\"Mean absolute error =\", round(mad, 2))\n",
    "    print(\"Mean squared error =\", round(mse, 2))\n",
    "    print(\"Median absolute error =\", round(med, 2))\n",
    "    print(\"R2 score =\", round(r2, 2))\n",
    "    print(\"Root Mean Squared Logarithmic Error =\",rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameteroptimierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für ein [Multi Layer Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) soll eine Hyperparameteroptimierung durchgeführt werden. Ziel ist es innerhalb der unten vorgegebenen Wertebereiche für die Hyperparameter `hidden_layer_sizes`, `activation` und `learning_rate` die beste Konfiguration zu finden. Hierzu kann entweder [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) oder [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) eingesetzt werden. GridSearchCV testet einfach alle Konfigurationen durch, benötigt daher aber viel Zeit. RandomizedSearchCV geht heuristisch und damit schneller durch den Suchraum. Wenden Sie eines dieser beiden Verfahren an, um für das unten gegebene Parameter-Grid die optimale Konfiguration zu finden. Welches ist die optimale Konfiguration und zu welchem `neg_mean_absolute_error` führt diese wenn man das scoring argument der Funktion entsprechend einstellt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'hidden_layer_sizes': [(10,),(20,),(30,),(40,),(50,),(100,),(10,10)], \n",
    "               'activation': [\"logistic\", \"tanh\", \"relu\"], \n",
    "               'learning_rate': [\"constant\", \"invscaling\", \"adaptive\"]}]\n",
    "param_grid"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "fahrzeug_segmentierung_uebung",
   "notebookOrigID": 628697663431637,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "DMaPR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
